{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarathSabu/Advanced-Prompt-Engineering/blob/main/Creating_ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7lFmcTzzTud"
      },
      "source": [
        "## Setup\n",
        "### Load the API key and relevant Python libaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVzKUnSpzLyY",
        "outputId": "24aa8bd0-324b-4f45-fde2-f1e17c1492ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.75.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "import openai\n",
        "from IPython.display import Markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spm6-Wwj1_Su",
        "outputId": "ee6f7ebc-bf05-4e67-9ef8-62e9b57740c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2l-BYOFp2bme"
      },
      "outputs": [],
      "source": [
        "api_key_path = \"/content/drive/MyDrive/Infor452/openai_key.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxgHdEXS2ybD"
      },
      "outputs": [],
      "source": [
        "def load_api_key(filepath):\n",
        "    try:\n",
        "        with open(filepath, 'r') as file:\n",
        "            return file.read().strip()  # Strip any extra whitespace or newline characters\n",
        "    except FileNotFoundError:\n",
        "        print(\"API key file not found. Please check the path.\")\n",
        "        return None\n",
        "\n",
        "# Load the API key\n",
        "OPENAI_API_KEY = load_api_key(api_key_path)\n",
        "if OPENAI_API_KEY is None:\n",
        "    raise ValueError(\"No API key found. Please check the file path or upload the file.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CttVrPsOwUNr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42021f51-0f04-497b-ba44-c8a4851bf741"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.72.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#If you have trouble with previous steps, ignore the previous steps and just copy & paste your keys here\n",
        "!pip install openai\n",
        "import openai\n",
        "from IPython.display import Markdown\n",
        "OPENAI_API_KEY = \"COPY&PASTEYOURAPIKEYHERE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PW77l8uyE5IJ"
      },
      "outputs": [],
      "source": [
        "client = openai.OpenAI(\n",
        "    api_key=OPENAI_API_KEY,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZCVo8rKj5iO"
      },
      "source": [
        "##If you want to get a sense of your tokens number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-TJXyLe9SPt"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    \"\"\"\n",
        "    Generates a completion for the given prompt using OpenAI's chat model.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The user's input or query.\n",
        "        model (str): The model to use (default is \"gpt-3.5-turbo\").\n",
        "\n",
        "    Returns:\n",
        "        str: The model's response as a string.\n",
        "    \"\"\"\n",
        "    # Create a list of messages for the chat completion API\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    # Generate the completion\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "    # Extract and return the content of the response\n",
        "    content = completion.choices[0].message\n",
        "    #add below for the token counts\n",
        "    token_dict = {\n",
        "        'prompt_tokens':completion.usage.prompt_tokens,\n",
        "        'completion_tokens':completion.usage.completion_tokens,\n",
        "        'total_tokens':completion.usage.total_tokens\n",
        "    }\n",
        "    return content, token_dict\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiuxWHnlCiEs",
        "outputId": "3aa8fb6f-ddbd-4d17-da40-61fdf366759c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant\n",
            "Prompt engineering involves crafting natural language instructions to optimize the output of a generative artificial intelligence model by providing specific queries, commands, context, and style for the AI to follow.\n",
            "{'prompt_tokens': 132, 'completion_tokens': 36, 'total_tokens': 168}\n"
          ]
        }
      ],
      "source": [
        "#we use backslashes (\\) are used for line continuation in Python to aviod including newlines (\\n) in the final string,\n",
        "\n",
        "text = \"\"\"\n",
        "Prompt engineering is the process of structuring or crafting an instruction in order to produce the best possible output from a generative artificial intelligence (AI) model.\\\n",
        "A prompt is natural language text describing the task that an AI should perform.\\\n",
        "A prompt for a text-to-text language model can be a query, a command, or a longer statement including context, instructions, and conversation history.\\\n",
        "Prompt engineering may involve phrasing a query, specifying a style, choice of words and grammar,[3] providing relevant context, or describing a character for the AI to mimic.\\\n",
        "\"\"\"\n",
        "prompt = f\"\"\"\n",
        "Summarize the text delimited by | into a single sentence.\n",
        "|{text}|\n",
        "\"\"\"\n",
        "response, token_dict = get_completion(prompt)\n",
        "print(response.role)\n",
        "print(response.content)\n",
        "print(token_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3oq7AbBkAhQ"
      },
      "source": [
        "#Many functions\n",
        "*   Summarization\n",
        "*   Question Answering\n",
        "*   Text Classification\n",
        "*   Role Playing\n",
        "*   Code Generation & Vulnerability Detection\n",
        "*   Website & Data structuring\n",
        "*   Customer service & sales & marketing\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fXenyiuki6O"
      },
      "source": [
        "#Zero-shot prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0Yf4Hi8Z06v",
        "outputId": "50cbf054-9727-4271-f5e6-7ea91d074f6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant\n",
            "Positive\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Decide whether a Tweet's sentiment is positive, neutral, or negative.\n",
        "\n",
        "Tweet: I loved the new YouTube video you made!\n",
        "Sentiment:\n",
        "\"\"\"\n",
        "response, token_dict = get_completion(prompt)\n",
        "print(response.role)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDx-FyOElfyO"
      },
      "source": [
        "#Few-shot prompt (In context learning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anCw49LClfA4"
      },
      "outputs": [],
      "source": [
        "def get_completion_adjustpara(prompt, model=\"gpt-3.5-turbo\", parameters=None, custom_messages=None):\n",
        "    # Use custom messages if provided, otherwise use the default\n",
        "    if custom_messages:\n",
        "        messages = custom_messages\n",
        "    else:\n",
        "        # Default messages\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are an AI assistant. You use a tone that is technical and scientific.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "        ]\n",
        "\n",
        "    # Define the default parameters\n",
        "    completion_params = {\n",
        "        \"model\": model,\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 1,  # Adjust randomness\n",
        "        \"max_tokens\": 256,  # Upper limit for generated tokens\n",
        "        \"top_p\": 1,  # Use nucleus sampling\n",
        "        \"frequency_penalty\": 0,  # Penalize repetition (-2.0 to 2.0)\n",
        "        \"presence_penalty\": 0,  # Penalize introducing new topics (-2.0 to 2.0)\n",
        "    }\n",
        "\n",
        "    # Update the default parameters with any custom parameters passed in\n",
        "    if parameters:\n",
        "        completion_params.update(parameters)\n",
        "    #print(completion_params)\n",
        "\n",
        "\n",
        "    completion = client.chat.completions.create(**completion_params)\n",
        "        # Extract and return the content of the response\n",
        "    return completion.choices[0].message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-LQwIoUqj7N",
        "outputId": "83659477-a21a-4369-e3fc-be3fc083739b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant\n",
            "False. The sum of odd numbers will always result in an odd number. In this case, the sum of the odd numbers 15, 5, 13, and 7 is 40, which is an even number.\n"
          ]
        }
      ],
      "source": [
        "# Define the prompt\n",
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "Is this statement True or False?\"\"\"\n",
        "\n",
        "# Define custom parameters\n",
        "custom_parameters = {\n",
        "    \"temperature\": 0,\n",
        "    \"max_tokens\": 500,\n",
        "    \"top_p\": 0.2\n",
        "}\n",
        "\n",
        "# Call the function\n",
        "response = get_completion_adjustpara(\n",
        "    prompt=prompt,  # Pass the prompt\n",
        "    parameters=custom_parameters  # Pass the custom parameters\n",
        ")\n",
        "\n",
        "# Print the response\n",
        "if response:\n",
        "    print(response.role)  # Should output 'assistant'\n",
        "    print(response.content)  # The content of the response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "dDpJGaGDr3y-",
        "outputId": "76e57abe-1a5f-4ca1-8d88-d551aaba869b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant\n",
            "The answer is True.\n"
          ]
        },
        {
          "data": {
            "text/markdown": "The answer is True.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Define the prompt\n",
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
        "A: The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
        "A: The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
        "A: The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
        "A: The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "A:\"\"\"\n",
        "\n",
        "# Define custom parameters\n",
        "custom_parameters = {\n",
        "    \"temperature\": 0,\n",
        "    \"max_tokens\": 50,\n",
        "    \"top_p\": 0.2\n",
        "}\n",
        "\n",
        "# Call the function\n",
        "response = get_completion_adjustpara(\n",
        "    prompt=prompt,  # Pass the prompt\n",
        "    parameters=custom_parameters  # Pass the custom parameters\n",
        ")\n",
        "\n",
        "# Print the response\n",
        "if response:\n",
        "    print(response.role)  # Should output 'assistant'\n",
        "    print(response.content)\n",
        "    display(Markdown(response.content)) # The content of the response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtiESpfqt8Te",
        "outputId": "b0072f2d-7516-4109-e20e-7eff1cc43230"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant\n",
            "Positive\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
        "Sentiment: Negative\n",
        "\n",
        "Message: Can't wait to order pizza for dinner tonight\n",
        "Sentiment: Positive\n",
        "\n",
        "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
        "Sentiment: ?\n",
        "\n",
        "Give a one word response.\n",
        "\"\"\"\n",
        "\n",
        "custom_parameters = {\n",
        "    \"temperature\": 1,\n",
        "    \"max_tokens\": 50,\n",
        "}\n",
        "\n",
        "# Call the function\n",
        "response = get_completion_adjustpara(\n",
        "    prompt=prompt,  # Pass the prompt\n",
        "    parameters=custom_parameters  # Pass the custom parameters\n",
        ")\n",
        "\n",
        "# Print the response\n",
        "if response:\n",
        "    print(response.role)  # Should output 'assistant'\n",
        "    print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4__umOLuShl"
      },
      "source": [
        "#Chain-of-thought prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czGX9t8Puc9B",
        "outputId": "a4f8970c-f64c-4871-9c88-343bbed08cd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant\n",
            "Based on the scenario provided, it is possible for all 15 individuals to get to the restaurant using cars and motorcycles collectively. The two individuals with cars can accommodate 5 people each, providing a total seating capacity of 10 people. Additionally, the two individuals with motorcycles can transport 4 people in total. Therefore, by utilizing the two cars and two motorcycles available, all 15 individuals can reach the restaurant via these vehicles.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "15 of us want to go to a restaurant.\n",
        "Two of them have cars\n",
        "Each car can seat 5 people.\n",
        "Two of us have motorcycles.\n",
        "Each motorcycle can fit 2 people.\n",
        "\n",
        "Can we all get to the restaurant by car or motorcycle?\n",
        "\"\"\"\n",
        "\n",
        "custom_parameters = {\n",
        "    \"temperature\": 1.5,\n",
        "    \"max_tokens\": 256,\n",
        "}\n",
        "response = get_completion_adjustpara(\n",
        "    prompt=prompt,  # Pass the prompt\n",
        "    parameters=custom_parameters  # Pass the custom parameters\n",
        ")\n",
        "\n",
        "# Print the response\n",
        "if response:\n",
        "    print(response.role)  # Should output 'assistant'\n",
        "    print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vhupfHKxoAc"
      },
      "source": [
        "#CoT Zero-shot: Lets think step by step\n",
        "Many LLMs already embed this within their systems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OM72bES-3hz",
        "outputId": "0f483fb5-e0c2-4166-80ac-495a72c8dfc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant\n",
            "Let's break down the scenario step by step:\n",
            "\n",
            "1. Initially, you bought 10 apples.\n",
            "2. You gave 2 apples to the neighbor, leaving you with 10 - 2 = 8 apples.\n",
            "3. You gave 2 more apples to the repairman, resulting in 8 - 2 = 6 apples remaining.\n",
            "4. You then bought 5 more apples, bringing the total to 6 + 5 = 11 apples.\n",
            "5. After eating 1 apple, you are left with 11 - 1 = 10 apples.\n",
            "\n",
            "Therefore, you remained with 10 apples after the series of transactions.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
        "\"\"\"\n",
        "custom_parameters = {\n",
        "    \"temperature\": 0,\n",
        "    \"max_tokens\": 256,\n",
        "}\n",
        "response = get_completion_adjustpara(\n",
        "    prompt=prompt,  # Pass the prompt\n",
        "    parameters=custom_parameters  # Pass the custom parameters\n",
        ")\n",
        "\n",
        "# Print the response\n",
        "if response:\n",
        "    print(response.role)  # Should output 'assistant'\n",
        "    print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yedpBckL_9h9",
        "outputId": "399cbfe8-5024-4db4-a072-aaac4603ddc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant\n",
            "Based on the information provided, there are a total of 15 individuals who want to go to the restaurant. \n",
            "\n",
            "Let's calculate the seating capacity available with cars and motorcycles:\n",
            "\n",
            "2 cars x 5 seats per car = 10 total seats available in cars\n",
            "2 motorcycles x 2 seats per motorcycle = 4 total seats available on motorcycles\n",
            "\n",
            "The total available seats with both cars and motorcycles are 10 + 4 = 14.\n",
            "\n",
            "Since the total number of individuals is 15 and the total seats available are 14, not all 15 could fit in cars or motorcycles simultaneously. However, you can consider viable alternatives such as carpooling, taking some individuals in the inefficient mode(like one extra on car to make couple of back and forth trips).\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "15 of us want to go to a restaurant.\n",
        "Two of them have cars\n",
        "Each car can seat 5 people.\n",
        "Two of us have motorcycles.\n",
        "Each motorcycle can fit 2 people.\n",
        "\n",
        "Can we all get to the restaurant by car or motorcycle?\n",
        "\n",
        "\"\"\"\n",
        "custom_parameters = {\n",
        "    \"temperature\": 1.5,\n",
        "    \"max_tokens\": 256,\n",
        "}\n",
        "response = get_completion_adjustpara(\n",
        "    prompt=prompt,  # Pass the prompt\n",
        "    parameters=custom_parameters  # Pass the custom parameters\n",
        ")\n",
        "\n",
        "# Print the response\n",
        "if response:\n",
        "    print(response.role)  # Should output 'assistant'\n",
        "    print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVjeyQGJwAD6",
        "outputId": "08d5e004-bd55-4ef7-fb74-a6e832556df3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant\n",
            "Let's break this down step by step:\n",
            "\n",
            "1. There are 15 people in total.\n",
            "2. Two of them have cars, and each car can seat 5 people. So, the two cars can accommodate 10 people in total.\n",
            "3. Two of them have motorcycles, and each motorcycle can fit 2 people. So, the two motorcycles can accommodate 4 people in total.\n",
            "4. Therefore, the total number of people that can be accommodated by cars and motorcycles is 10 (from cars) + 4 (from motorcycles) = 14 people.\n",
            "\n",
            "Based on the given information, it appears that not all 15 people can get to the restaurant by car or motorcycle. One person would not have transportation in this scenario.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "15 of us want to go to a restaurant.\n",
        "Two of them have cars\n",
        "Each car can seat 5 people.\n",
        "Two of us have motorcycles.\n",
        "Each motorcycle can fit 2 people.\n",
        "\n",
        "Can we all get to the restaurant by car or motorcycle?\n",
        "\n",
        "Think step by step.\n",
        "\"\"\"\n",
        "custom_parameters = {\n",
        "    \"temperature\": 0,\n",
        "    \"max_tokens\": 256,\n",
        "}\n",
        "response = get_completion_adjustpara(\n",
        "    prompt=prompt,  # Pass the prompt\n",
        "    parameters=custom_parameters  # Pass the custom parameters\n",
        ")\n",
        "\n",
        "# Print the response\n",
        "if response:\n",
        "    print(response.role)  # Should output 'assistant'\n",
        "    print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uh5b4ZBlxl2r",
        "outputId": "9b592fff-ffad-4aed-f207-abcdd0170402"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant\n",
            "Let's break down the problem step by step:\n",
            "\n",
            "1. Total number of people = 15\n",
            "2. Number of cars available = 2\n",
            "3. Number of people each car can seat = 5\n",
            "4. Total seating capacity of cars = 2 cars * 5 people/car = 10 people\n",
            "5. Number of motorcycles available = 2\n",
            "6. Number of people each motorcycle can fit = 2\n",
            "7. Total seating capacity of motorcycles = 2 motorcycles * 2 people/motorcycle = 4 people\n",
            "\n",
            "Now, let's calculate the total seating capacity of both cars and motorcycles combined:\n",
            "\n",
            "Total seating capacity = Seating capacity of cars + Seating capacity of motorcycles\n",
            "Total seating capacity = 10 people (from cars) + 4 people (from motorcycles) = 14 people\n",
            "\n",
            "Since the total seating capacity of both cars and motorcycles combined is 14 people, which is less than the total number of people (15), not all 15 people can get to the restaurant by car or motorcycle. \n",
            "\n",
            "Therefore, based on the seating capacity of the available vehicles, not all 15 people can get to the restaurant by car or motorcycle.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "15 of us want to go to a restaurant.\n",
        "Two of them have cars\n",
        "Each car can seat 5 people.\n",
        "Two of us have motorcycles.\n",
        "Each motorcycle can fit 2 people.\n",
        "\n",
        "Can we all get to the restaurant by car or motorcycle?\n",
        "\n",
        "Think step by step.\n",
        "Explain each intermediate step.\n",
        "Only when you are done with all your steps,\n",
        "provide the answer based on your intermediate steps.\n",
        "\"\"\"\n",
        "\n",
        "custom_parameters = {\n",
        "    \"temperature\": 0,\n",
        "    \"max_tokens\": 256,\n",
        "}\n",
        "response = get_completion_adjustpara(\n",
        "    prompt=prompt,  # Pass the prompt\n",
        "    parameters=custom_parameters  # Pass the custom parameters\n",
        ")\n",
        "\n",
        "# Print the response\n",
        "if response:\n",
        "    print(response.role)  # Should output 'assistant'\n",
        "    print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qysgSGBDCR-"
      },
      "source": [
        "#Cot with few-shot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whKRNdUEJZ0t"
      },
      "source": [
        "##Chain of Thought Best Practices\n",
        "####Don't Use a small LLM.\n",
        "####Do Put the answer after the chain-of-thought reasoning, not before.\n",
        "####Do Set temperature to 0. (You can try with different temp to find the best reasoning process)\n",
        "####Do Use few-shot chain of thought, not just one-shot or zero-shot.\n",
        "####Do Write exemplars that include everything you would say when talking through the reasoning step-by-step\n",
        "####Don't Use math equations in place of natural language reasoning.\n",
        "####Chain of thought requires natural language reasoning.\n",
        "####Don't Assume chain of thought stops hallucinations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3-tH_zH-dxd",
        "outputId": "78012c0d-c6eb-43f1-f3dd-0011659d1a7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant\n",
            "To calculate the total number of computers in the server room after the installations from Monday to Thursday, we first determine the number of computers installed each day. Since five computers were installed each day for four days (Monday to Thursday), the total number of computers installed is 5 * 4 = 20.\n",
            "\n",
            "Adding the initial nine computers to the newly installed 20 computers, the total number of computers in the server room now is 9 + 20 = 29. Therefore, there are now 29 computers in the server room.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Q:There are 15 trees in thegrove.Grove workers will plant trees in the grove today.After they are done,there will be 21 trees.How many trees did the grove workers plant today?\n",
        "A:There are 15 trees originally.Then there were 21 trees after some more were planted.So there must have been 21-15=6.The answer is 6.\n",
        "\n",
        "Q:If there are 3 cars in the parking lot and 2 more cars arrive,how many cars are in the parking lot?\n",
        "A:There are originally 3 cars.2 more cars arrive.3+2=5.The answer is 5.\n",
        "\n",
        "Q:Jason had 20 lollipops.He gave Denny some lollipops.Now Jason has 12 lollipops.How many lollipops did Jason give to Denny?\n",
        "A:Jason started with 20 lollipops.Then he had 12 after giving some to Denny.So he gave Denny 20-12=8. The answer is 8.\n",
        "\n",
        "Q:There were nine computers in the server room.Five more computers were installed each day,from monday to thursday.How many computers are now in the server room?\n",
        "A:\n",
        "\n",
        "\"\"\"\n",
        "#Follow the format\n",
        "custom_parameters = {\n",
        "    \"temperature\": 0,\n",
        "    \"max_tokens\": 256,\n",
        "}\n",
        "response = get_completion_adjustpara(\n",
        "    prompt=prompt,  # Pass the prompt\n",
        "    parameters=custom_parameters  # Pass the custom parameters\n",
        ")\n",
        "\n",
        "# Print the response\n",
        "if response:\n",
        "    print(response.role)  # Should output 'assistant'\n",
        "    print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93osAVoG7oXz"
      },
      "source": [
        "##Meta prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "B99Al2eaGDzq",
        "outputId": "e2d5e365-48fd-404c-faed-90e7c22af4e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant\n"
          ]
        },
        {
          "data": {
            "text/markdown": "To solve the quadratic equation \\(3x^2 + 4x - 5 = 0\\), we can use the quadratic formula:\n\nThe quadratic formula states that for an equation of the form \\(ax^2 + bx + c = 0\\), the solutions for \\(x\\) are given by:\n\n\\[x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\\]\n\nIn this case, the coefficients are \\(a = 3\\), \\(b = 4\\), and \\(c = -5\\). Substituting these values into the formula, we get:\n\n\\[x = \\frac{-4 \\pm \\sqrt{4^2 - 4 \\cdot 3 \\cdot (-5)}}{2 \\cdot 3}\\]\n\n\\[x = \\frac{-4 \\pm \\sqrt{16 + 60}}{6}\\]\n\n\\[x = \\frac{-4 \\pm \\sqrt{76}}{6}\\]\n\n\\[x = \\frac{-4 \\pm 2\\sqrt{19}}{6}\\]\n\nNow, we have two possible solutions:\n\n1. \\(x = \\frac{-4 + 2\\sqrt{19}}{6} = \\frac{-2 + \\sqrt{19}}{3}\\)\n2. \\(x = \\frac{-4 - 2\\sqrt{19}}{6} = \\frac{-2 - \\sqrt{19}}{3}\\)\n\nTherefore, the solutions to the quadratic equation \\(3x^2 + 4x - 5 = 0\\) are \\(x = \\frac{-2 + \\sqrt{19}}{3}\\) and \\(x = \\frac{-",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "prompt = \"\"\"\n",
        "You are ChatGPT, a highly advanced large language model with specialized expertise in mathematics. Your core strengths lie in tackling complex mathematical challenges,\n",
        "utilizing intricate reasoning, and delivering solutions through methodical problem-solving.\n",
        "Your primary objective is to analyze and solve each mathematical problem in a rigorous and detailed manner. To achieve this, follow these steps:\n",
        "1. Clearly identify and understand the problem statement provided by the user.\n",
        "2. Break down the problem into manageable components, if applicable.\n",
        "3. Apply relevant mathematical principles, formulas, or techniques to address each component.\n",
        "4. Synthesize the solutions to the components to formulate a comprehensive answer.\n",
        "5. Provide a clear, step-by-step explanation of your reasoning, ensuring thoroughness and precision.\n",
        "\n",
        "\n",
        "Your expertise spans a variety of mathematical domains, including but not limited to:\n",
        "- Basic arithmetic and number theory\n",
        "- Algebra and geometry\n",
        "- Calculus and differential equations\n",
        "- Probability and statistics\n",
        "- Linear algebra, group theory, and other advanced topics\n",
        "\n",
        "Ensure your explanations are easy to follow, even for those without advanced mathematical training, while maintaining accuracy and rigor.\n",
        "Be concise when appropriate but thorough in addressing the key aspects of the problem.\n",
        "\n",
        "\n",
        "Problem: Solve the quadratic equation 3x^2 + 4x âˆ’ 5 = 0.\n",
        "\"\"\"\n",
        "\n",
        "custom_parameters = {\n",
        "    \"temperature\": 0,\n",
        "    \"max_tokens\": 350,\n",
        "}\n",
        "\n",
        "response = get_completion_adjustpara(\n",
        "    prompt=prompt,  # Pass the completed prompt\n",
        "    parameters=custom_parameters  # Pass the custom parameters\n",
        ")\n",
        "\n",
        "# Print the response\n",
        "if response:\n",
        "    print(response.role)  # Should output 'assistant'\n",
        "    display(Markdown(response.content))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY8sKdk9fN3Z"
      },
      "source": [
        "## Self-Consistency\n",
        "\n",
        "Self-Consistency is a technique to improve the performance of chain of thought prompts--you make the same LLM call multiple times and take the most common answer.\n",
        "\n",
        "This means \"breaking\" the rule to use chain of thought with temperature=0.\n",
        "\n",
        "The intuition behind self-consistency is:\n",
        "1. Multiple responses to identical LLM calls means a variety of reasoning paths in the responses.\n",
        "1. Incorrect reasoning paths lead to different incorrect answers.\n",
        "1. Correct reasoning paths lead to the same correct answer.\n",
        "1. While you may only get a few correct answers and many incorrect answers, the correct answer will be more common than any unique incorrect answer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "_hx0X21l-QX-",
        "outputId": "5e080550-3186-481e-cb65-9a13bd988540"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant\n"
          ]
        },
        {
          "data": {
            "text/markdown": "We need to calculate the total units produced by Megacorp's factories.\n\nFirst, determine the number of factories operating at different productivity levels:\n- Factories being upgraded: 3 factories * 25% = 0.25 * 100 units = 25 units per factory\n- Factories under maintenance: 2 factories * 50% = 0.50 * 100 units = 50 units per factory\n- Factory under labor action: 0 units\n\nNext, calculate the total units produced by each type of factory:\n- Upgraded factories: 3 factories * 25 units = 75 units\n- Factories under maintenance: 2 factories * 50 units = 100 units\n- Factories at baseline productivity: (19 total factories - 3 upgraded - 2 maintenance - 1 labor action) * 100 units = 13 * 100 units = 1300 units\n\nFinally, add up the total units produced by each type of factory to find the overall daily production:\n75 units (upgraded) + 100 units (maintenance) + 1300 units (baseline) + 0 units (labor action) = 1475 units\n\nTherefore, the answer is 1475 units.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#First, run this next LLM call with temperature 0 to generate an incorrect response.\n",
        "# The answer is 1300 + 100 (maintenance) + 75 (upgrade) = 1475.\n",
        "prompt = \"\"\"Answer questions showing the full math and reasoning.\n",
        "Follow the pattern in the example.\n",
        "\n",
        "Q: A regular tennis ball can holds 5 balls.\n",
        "A large tennis ball can holds 200% of a regular tennis ball can.\n",
        "A small tennis ball can holds 40% of a regular tennis ball can.\n",
        "A collectable tennis ball can holds no tennis balls.\n",
        "Roger has 10 tennis ball cans.\n",
        "3 cans are large cans.\n",
        "4 cans are small cans.\n",
        "1 can is collectable.\n",
        "How many tennis balls does Roger have?\n",
        "A: We need to find the number of regular tennis ball cans.\n",
        "Roger has 10 (total) - 3 (large) - 4 (small) - 1 (collectable) = 2 regular cans.\n",
        "A large tennis ball can holds 200% of 5 = 10 tennis balls.\n",
        "A small tennis ball can holds 40% of 5 = 2 tennis balls.\n",
        "Next count how many balls come from each can type.\n",
        "3 large cans is 3 * 10 = 30 tennis balls.\n",
        "4 small cans is 2 * 4 = 8 tennis balls.\n",
        "2 regular cans is 2 * 5 = 10 tennis balls\n",
        "1 collectable can is 0 tennis balls.\n",
        "To get the answer, add the number of balls from each can type.\n",
        "Roger has 30 (large) + 8 (small) + 10 (regular) + 0 (collectable) = 48 balls.\n",
        "The answer is 48.\n",
        "\n",
        "Q: Factories have a baseline productivity of 100 units per day.\n",
        "Not all factories have the baseline productivity.\n",
        "When a factory is being upgraded, it has 25% of the baseline productivity.\n",
        "When a factory is undergoing maintenance, it has 50% of the baseline.\n",
        "When a factory is under labor action, it produces nothing.\n",
        "Megacorp has 19 factories in total.\n",
        "3 factories are being upgraded.\n",
        "2 factories are under maintenance.\n",
        "1 is under labor action.\n",
        "How many units does megacorp produce in a day?\n",
        "The final answer needs to be answered with\n",
        "The answer is \"\"\"\n",
        "custom_parameters = {\n",
        "    \"temperature\": 0,\n",
        "    \"max_tokens\": 350,\n",
        "}\n",
        "\n",
        "response = get_completion_adjustpara(\n",
        "    prompt=prompt,  # Pass the completed prompt\n",
        "    parameters=custom_parameters  # Pass the custom parameters\n",
        ")\n",
        "\n",
        "# Print the response\n",
        "if response:\n",
        "    print(response.role)  # Should output 'assistant'\n",
        "    display(Markdown(response.content))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4fsM36WDqFy"
      },
      "outputs": [],
      "source": [
        "custom_messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are ChatGPT, a highly advanced large language model with specialized expertise in mathematics. Your core strengths lie in tackling complex mathematical challenges,\\\n",
        "                utilizing intricate reasoning, and delivering solutions through methodical problem-solving. For each question, your final soluction should be end with The answer is \"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6gQXuaSAfVp",
        "outputId": "8f78206f-d213-46c1-a0b4-9acf16ce9347"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response 1...\n",
            "Q: Factories have a baseline productivity of 100 units per day.\n",
            "Not all factories have the baseline productivity.\n",
            "When a factory is being upgraded, it has 25% of the baseline productivity.\n",
            "When a factory is undergoing maintenance, it has 50% of the baseline.\n",
            "When a factory is under labor action, it produces nothing.\n",
            "Megacorp has 19 factories in total.\n",
            "3 factories are being upgraded.\n",
            "2 factories are under maintenance.\n",
            "1 is under labor action.\n",
            "How many units does Megacorp produce in a day?\n",
            "\n",
            "A: First, calculate the number of factories operating at different productivity levels.\n",
            "Factories being upgraded: 3 factories * 25% = 0.25 * 100 = 25 units per factory\n",
            "Factories under maintenance: 2 factories * 50% = 0.5 * 100 = 50 units per factory\n",
            "Factories under labor action: 0 units per factory\n",
            "\n",
            "Now, calculate the total units produced by each type of factory:\n",
            "Factories being upgraded: 3 factories * 25 units = 75 units\n",
            "Factories under maintenance: 2 factories * 50 units = 100 units\n",
            "Factories under labor action: 0 units\n",
            "\n",
            "Next, find the number of factories operating at baseline productivity:\n",
            "Total factories - (Factories being upgraded + Factories under maintenance + Factories under labor action)\n",
            "19 - (3 + 2 + 1) = 19 - 6 = 13 factories\n",
            "\n",
            "Now, calculate the total units produced by factories operating at baseline productivity:\n",
            "13 factories * 100 units = 1300 units\n",
            "\n",
            "Finally, add up the total units produced by each type of factory to find Megacorp's total daily production:\n",
            "75\n",
            "Response 2...\n",
            "Q: Factories have a baseline productivity of 100 units per day.\n",
            "Not all factories have the baseline productivity.\n",
            "When a factory is being upgraded, it has 25% of the baseline productivity.\n",
            "When a factory is undergoing maintenance, it has 50% of the baseline.\n",
            "When a factory is under labor action, it produces nothing.\n",
            "Megacorp has 19 factories in total.\n",
            "3 factories are being upgraded.\n",
            "2 factories are under maintenance.\n",
            "1 is under labor action.\n",
            "How many units does Megacorp produce in a day?\n",
            "\n",
            "A: First, calculate the number of factories operating at different productivity levels.\n",
            "Factories being upgraded: 3 * 25% = 0.25 * 100 = 25 units per factory.\n",
            "Factories under maintenance: 2 * 50% = 0.5 * 100 = 50 units per factory.\n",
            "Factories under labor action: 0 units per factory.\n",
            "\n",
            "Next, determine the number of factories operating normally:\n",
            "Total factories - (factories being upgraded + factories under maintenance + factories under labor action)\n",
            "19 - (3 + 2 + 1) = 19 - 6 = 13 factories.\n",
            "\n",
            "Since normal factories operate at 100 units per day, the total units produced by normal factories are:\n",
            "13 * 100 = 1300 units.\n",
            "\n",
            "Now, calculate the total units produced by factories at different productivity levels:\n",
            "Factories being upgraded: 3 * 25 = 75 units.\n",
            "Factories under maintenance: 2 * 50 = 100 units.\n",
            "Factories under labor action: 0 units.\n",
            "\n",
            "Finally, add up the total units produced by all factories:\n",
            "1300 (normal) + 75 (upgraded) + 100 (maintenance\n",
            "Response 3...\n",
            "Q: We need to find the total units produced by Megacorp in a day.\n",
            "Megacorp has 19 factories in total.\n",
            "3 factories are being upgraded, which produce 25% of the baseline productivity, so each produces 25 units per day.\n",
            "2 factories are under maintenance, which produce 50% of the baseline productivity, so each produces 50 units per day.\n",
            "1 factory is under labor action, which produces nothing.\n",
            "To calculate the total units produced:\n",
            "Total units from upgraded factories = 3 factories * 25 units/factory = 75 units\n",
            "Total units from maintenance factories = 2 factories * 50 units/factory = 100 units\n",
            "Total units from labor action factories = 0 units\n",
            "Total units produced by Megacorp = 75 units + 100 units + 0 units = 175 units\n",
            "The answer is 175.\n",
            "Response 4...\n",
            "Q: We need to find the total units produced by Megacorp in a day.\n",
            "Megacorp has 19 factories in total.\n",
            "3 factories are being upgraded, which produce 25% of the baseline productivity, so each produces 25 units per day.\n",
            "2 factories are under maintenance, which produce 50% of the baseline productivity, so each produces 50 units per day.\n",
            "1 factory is under labor action, which produces nothing.\n",
            "To calculate the total units produced:\n",
            "Total units from upgraded factories = 3 factories * 25 units/factory = 75 units\n",
            "Total units from maintenance factories = 2 factories * 50 units/factory = 100 units\n",
            "Total units from labor action factories = 0 units\n",
            "Total units produced by Megacorp = 75 units + 100 units + 0 units = 175 units\n",
            "The answer is 175.\n",
            "Response 5...\n",
            "Q: We need to find the total units produced by Megacorp in a day.\n",
            "Megacorp has 19 factories in total.\n",
            "3 factories are being upgraded, which produce 25% of the baseline productivity, so each produces 25 units per day.\n",
            "2 factories are under maintenance, which produce 50% of the baseline productivity, so each produces 50 units per day.\n",
            "1 factory is under labor action, which produces nothing.\n",
            "To calculate the total units produced:\n",
            "Total units from upgraded factories = 3 factories * 25 units/factory = 75 units\n",
            "Total units from maintenance factories = 2 factories * 50 units/factory = 100 units\n",
            "Total units from labor action factories = 0 units\n",
            "Total units produced by Megacorp = 75 units + 100 units + 0 units = 175 units\n",
            "The answer is 175.\n",
            "Response 6...\n",
            "Q: We need to find the total units produced by Megacorp in a day.\n",
            "Megacorp has 19 factories in total.\n",
            "3 factories are being upgraded, which produce 25% of the baseline productivity, so each produces 25 units per day.\n",
            "2 factories are under maintenance, which produce 50% of the baseline productivity, so each produces 50 units per day.\n",
            "1 factory is under labor action, which produces nothing.\n",
            "To calculate the total units produced:\n",
            "For the factories being upgraded: 3 factories * 25 units/factory = 75 units\n",
            "For the factories under maintenance: 2 factories * 50 units/factory = 100 units\n",
            "For the factory under labor action: 0 units\n",
            "Adding these up gives the total units produced by Megacorp in a day:\n",
            "75 (upgraded) + 100 (maintenance) + 0 (labor action) = 175 units.\n",
            "The answer is 175.\n",
            "Response 7...\n",
            "Q: Factories have a baseline productivity of 100 units per day.\n",
            "Not all factories have the baseline productivity.\n",
            "When a factory is being upgraded, it has 25% of the baseline productivity.\n",
            "When a factory is undergoing maintenance, it has 50% of the baseline.\n",
            "When a factory is under labor action, it produces nothing.\n",
            "Megacorp has 19 factories in total.\n",
            "3 factories are being upgraded.\n",
            "2 factories are under maintenance.\n",
            "1 is under labor action.\n",
            "How many units does Megacorp produce in a day?\n",
            "\n",
            "A: First, calculate the number of factories operating at different productivity levels.\n",
            "Factories not mentioned are assumed to be at baseline productivity.\n",
            "Factories being upgraded: 3 factories * 25 units = 75 units\n",
            "Factories under maintenance: 2 factories * 50 units = 100 units\n",
            "Factories under labor action: 0 units\n",
            "\n",
            "Now, calculate the total units produced by Megacorp.\n",
            "Total units produced = Baseline productivity * (Total factories - Factories being upgraded - Factories under maintenance)\n",
            "Total units produced = 100 units * (19 factories - 3 factories - 2 factories)\n",
            "Total units produced = 100 units * 14 factories\n",
            "Total units produced = 1400 units\n",
            "\n",
            "Therefore, Megacorp produces 1400 units in a day.\n",
            "The answer is 1400.\n",
            "Response 8...\n",
            "Q: Factories have a baseline productivity of 100 units per day.\n",
            "Not all factories have the baseline productivity.\n",
            "When a factory is being upgraded, it has 25% of the baseline productivity.\n",
            "When a factory is undergoing maintenance, it has 50% of the baseline.\n",
            "When a factory is under labor action, it produces nothing.\n",
            "Megacorp has 19 factories in total.\n",
            "3 factories are being upgraded.\n",
            "2 factories are under maintenance.\n",
            "1 is under labor action.\n",
            "How many units does Megacorp produce in a day?\n",
            "\n",
            "A: First, calculate the number of factories operating at different productivity levels.\n",
            "Factories being upgraded: 3 factories * 25 units = 75 units\n",
            "Factories under maintenance: 2 factories * 50 units = 100 units\n",
            "Factories under labor action: 0 units (as they produce nothing)\n",
            "Remaining factories: 19 total - 3 upgraded - 2 maintenance - 1 labor action = 13 factories\n",
            "\n",
            "Now, calculate the total units produced by the remaining factories:\n",
            "13 factories * 100 units = 1300 units\n",
            "\n",
            "Finally, add up the units produced by each category to find the total units produced by Megacorp in a day:\n",
            "75 units (upgraded) + 100 units (maintenance) + 0 units (labor action) + 1300 units (remaining) = 1475 units\n",
            "\n",
            "Therefore, Megacorp produces 1475 units in a day.\n",
            "The answer is 1475.\n",
            "Response 9...\n",
            "Q: Factories have a baseline productivity of 100 units per day.\n",
            "Not all factories have the baseline productivity.\n",
            "When a factory is being upgraded, it has 25% of the baseline productivity.\n",
            "When a factory is undergoing maintenance, it has 50% of the baseline.\n",
            "When a factory is under labor action, it produces nothing.\n",
            "Megacorp has 19 factories in total.\n",
            "3 factories are being upgraded.\n",
            "2 factories are under maintenance.\n",
            "1 is under labor action.\n",
            "How many units does Megacorp produce in a day?\n",
            "\n",
            "A: First, calculate the number of factories operating at different productivity levels.\n",
            "Factories being upgraded: 3 factories * 25 units = 75 units\n",
            "Factories under maintenance: 2 factories * 50 units = 100 units\n",
            "Factories under labor action: 0 units (produces nothing)\n",
            "Remaining factories: 19 total - 3 upgraded - 2 maintenance - 1 labor action = 13 factories\n",
            "\n",
            "Now, calculate the total units produced by the remaining factories:\n",
            "13 factories * 100 units = 1300 units\n",
            "\n",
            "Finally, add up the units produced by each category to find the total units produced by Megacorp in a day:\n",
            "75 units (upgraded) + 100 units (maintenance) + 1300 units (remaining) + 0 units (labor action) = 1475 units\n",
            "\n",
            "Therefore, Megacorp produces 1475 units in a day.\n",
            "The answer is 1475.\n",
            "Response 10...\n",
            "Q: Factories have a baseline productivity of 100 units per day.\n",
            "Not all factories have the baseline productivity.\n",
            "When a factory is being upgraded, it has 25% of the baseline productivity.\n",
            "When a factory is undergoing maintenance, it has 50% of the baseline.\n",
            "When a factory is under labor action, it produces nothing.\n",
            "Megacorp has 19 factories in total.\n",
            "3 factories are being upgraded.\n",
            "2 factories are under maintenance.\n",
            "1 is under labor action.\n",
            "How many units does Megacorp produce in a day?\n",
            "\n",
            "A: First, calculate the number of factories operating at different productivity levels.\n",
            "Factories not mentioned are assumed to be at baseline productivity.\n",
            "\n",
            "Factories being upgraded: 3 factories * 25 units = 75 units\n",
            "Factories under maintenance: 2 factories * 50 units = 100 units\n",
            "Factories under labor action: 0 units\n",
            "\n",
            "Now, calculate the total units produced by Megacorp:\n",
            "Total units = Baseline units - (Units lost due to upgrades + Units lost due to maintenance)\n",
            "Total units = 19 factories * 100 units - (75 units + 100 units)\n",
            "Total units = 1900 units - 175 units\n",
            "Total units = 1725 units\n",
            "\n",
            "Therefore, Megacorp produces 1725 units in a day.\n",
            "The answer is 1725.\n",
            "\n",
            "Answers and counts from most common to least common:\n",
            "[('175', 4), ('NA', 2), ('1475', 2), ('1400', 1), ('1725', 1)]\n",
            "Q: We need to find the total units produced by Megacorp in a day.\n",
            "Megacorp has 19 factories in total.\n",
            "3 factories are being upgraded, which produce 25% of the baseline productivity, so each produces 25 units per day.\n",
            "2 factories are under maintenance, which produce 50% of the baseline productivity, so each produces 50 units per day.\n",
            "1 factory is under labor action, which produces nothing.\n",
            "To calculate the total units produced:\n",
            "Total units from upgraded factories = 3 factories * 25 units/factory = 75 units\n",
            "Total units from maintenance factories = 2 factories * 50 units/factory = 100 units\n",
            "Total units from labor action factories = 0 units\n",
            "Total units produced by Megacorp = 75 units + 100 units + 0 units = 175 units\n",
            "The answer is 175.\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter  # Easy counting of most common responses\n",
        "# Number of test runs\n",
        "sc_runs = 10\n",
        "# Initialize responses and answers\n",
        "responses = [None] * sc_runs\n",
        "answers = [None] * sc_runs\n",
        "sc_parameters = {\n",
        "    \"temperature\": 0,\n",
        "    \"max_tokens\": 512,\n",
        "}\n",
        "\n",
        "for i in range(sc_runs):\n",
        "    print(f\"Response {i + 1}...\")\n",
        "\n",
        "    # Call the LLM with parameters and prompt\n",
        "    responses[i] = get_completion_adjustpara(\n",
        "        prompt=None,\n",
        "        parameters=custom_parameters,\n",
        "        custom_messages=custom_messages\n",
        "    )\n",
        "\n",
        "    # Extract the answer from the response content\n",
        "    try:\n",
        "        # Attempt to extract \"The answer is ...\" from the response only works if the answer starts with The answer is and with numerical value\n",
        "        answers[i] = responses[i].content.split(\"The answer is\")[1].split(\".\")[0].strip()\n",
        "    except Exception as e:\n",
        "        # Default to \"NA\" if parsing fails\n",
        "        answers[i] = \"NA\"\n",
        "\n",
        "    # Print the full response for debugging\n",
        "    print(responses[i].content)\n",
        "\n",
        "# Analyze and display results\n",
        "print(\"\\nAnswers and counts from most common to least common:\")\n",
        "common_answers = Counter(answers).most_common()\n",
        "print(common_answers)\n",
        "\n",
        "most_common_answer = common_answers[0][0]  # E.g., '1475'\n",
        "\n",
        "# Find the full response for the most common answer\n",
        "for i, answer in enumerate(answers):\n",
        "    if answer == most_common_answer:\n",
        "        most_common_response = responses[i]  # Get the first matching response\n",
        "        break\n",
        "\n",
        "# Display the most common response as Markdown\n",
        "if most_common_response:\n",
        "    print(most_common_response.content)\n",
        "else:\n",
        "    print(\"No valid response found for the most common answer.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "wgH9Nbw3CCYB",
        "outputId": "4c679a22-6928-486e-d0d6-06d1eba57fc6"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(Counter(answers).keys(), Counter(answers).values())\n",
        "ax.tick_params(axis='x', rotation=55)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdz3vucuh-9A"
      },
      "source": [
        "##Generated Knowledge Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iq4urErnh-DX"
      },
      "outputs": [],
      "source": [
        "def get_completion_adjustpara(prompt, model=\"gpt-3.5-turbo\", parameters=None, custom_messages=None):\n",
        "    # Use custom messages if provided, otherwise use the default\n",
        "    if custom_messages:\n",
        "        messages = custom_messages\n",
        "    else:\n",
        "        # Default messages\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are an AI knowledeg base. Your task is to generate the scientific knowledge and facts.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "        ]\n",
        "\n",
        "    # Define the default parameters\n",
        "    completion_params = {\n",
        "        \"model\": model,\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 1,  # Adjust randomness\n",
        "        \"max_tokens\": 256,  # Upper limit for generated tokens\n",
        "        \"top_p\": 1,  # Use nucleus sampling\n",
        "        \"frequency_penalty\": 0,  # Penalize repetition (-2.0 to 2.0)\n",
        "        \"presence_penalty\": 0,  # Penalize introducing new topics (-2.0 to 2.0)\n",
        "    }\n",
        "\n",
        "    # Update the default parameters with any custom parameters passed in\n",
        "    if parameters:\n",
        "        completion_params.update(parameters)\n",
        "    #print(completion_params)\n",
        "\n",
        "\n",
        "    completion = client.chat.completions.create(**completion_params)\n",
        "        # Extract and return the content of the response\n",
        "    return completion.choices[0].message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "c-0x00cPuqJS",
        "outputId": "47844689-8384-4990-b784-401206060fed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant\n"
          ]
        },
        {
          "data": {
            "text/markdown": "Knowledge: In golf, the objective is to complete each hole in the fewest strokes possible. Unlike many other sports where a higher score indicates better performance, in golf, a lower score is better. The player with the lowest total number of strokes at the end of the round is the winner.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prompts = \"\"\"\n",
        "Input: Greece is larger than Mexico.\n",
        "Knowledge: Greece is approximately 131,957 sq km, while Mexico is approximately 1,964,375 sq km, making Mexico 1,389% larger than Greece.\n",
        "\n",
        "Input: A fish is capable of thinking.\n",
        "Knowledge: Fish are more intelligent than they appear. In many areas, such as memory, their cognitive powers match or exceed those of 'higher' vertebrates including non-human primates. Fishâ€™s long-term memories help them keep track of complex social relationships.\n",
        "\n",
        "Input: Part of golf is trying to get a higher point total than others.\n",
        "\"\"\"\n",
        "custom_parameters = {\n",
        "    \"temperature\": 0,\n",
        "    \"max_tokens\": 500,\n",
        "}\n",
        "\n",
        "response = get_completion_adjustpara(\n",
        "        prompt=prompts,\n",
        "        parameters=custom_parameters,\n",
        "    )\n",
        "\n",
        "# Print the response\n",
        "if response:\n",
        "    print(response.role)  # Should output 'assistant'\n",
        "    display(Markdown(response.content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSagOMGgyQ9M"
      },
      "outputs": [],
      "source": [
        "knowledge = \"In golf, the objective is to complete each hole in the fewest strokes possible. Unlike many other sports where a higher score indicates better performance, in golf, a lower score is better. The player with the lowest total number of strokes at the end of the round is the winner.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "9yAR66sZx0Vq",
        "outputId": "34c0c6a4-42fe-485f-cea6-a1e7f788c979"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant\n"
          ]
        },
        {
          "data": {
            "text/markdown": "No, that statement is not correct. In golf, the objective is to complete each hole in the fewest strokes possible. Unlike many other sports where a higher score indicates better performance, in golf, a lower score is better. The player with the lowest total number of strokes at the end of the round is the winner.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "custom_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are an AI assistant. Your task is to incorporate the knowledge delimited in ''' to answer the question.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"Part of golf is trying to get a higher point total than others. Is this correct, answered it based on the following knowledge'''{knowledge} '''\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Custom parameters for generation\n",
        "custom_parameters = {\n",
        "    \"temperature\": 0,  # Ensure deterministic output\n",
        "    \"max_tokens\": 500,\n",
        "}\n",
        "\n",
        "# Generate a response\n",
        "response = get_completion_adjustpara(\n",
        "    prompt=None,  # Prompt is handled through custom_messages\n",
        "    parameters=custom_parameters,\n",
        "    custom_messages=custom_messages\n",
        ")\n",
        "\n",
        "# Print the response\n",
        "if response:\n",
        "    print(response.role)  # Outputs 'assistant'\n",
        "    display(Markdown(response.content))  # Display the response in Markdown format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS7DFtNiMkx3"
      },
      "source": [
        "#The Chat Format (Memory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juHrSjuBMdsD"
      },
      "outputs": [],
      "source": [
        "def get_completion_adjustpara(prompt, model=\"gpt-3.5-turbo\", parameters=None, custom_messages=None):\n",
        "    # Use custom messages if provided, otherwise use the default\n",
        "    if custom_messages:\n",
        "        messages = custom_messages\n",
        "    else:\n",
        "        # Default messages\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are an AI assistant. You use a tone that is technical and scientific.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "        ]\n",
        "\n",
        "    # Define the default parameters\n",
        "    completion_params = {\n",
        "        \"model\": model,\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 1,  # Adjust randomness\n",
        "        \"max_tokens\": 256,  # Upper limit for generated tokens\n",
        "        \"top_p\": 1,  # Use nucleus sampling\n",
        "        \"frequency_penalty\": 0,  # Penalize repetition (-2.0 to 2.0)\n",
        "        \"presence_penalty\": 0,  # Penalize introducing new topics (-2.0 to 2.0)\n",
        "    }\n",
        "\n",
        "    # Update the default parameters with any custom parameters passed in\n",
        "    if parameters:\n",
        "        completion_params.update(parameters)\n",
        "    #print(completion_params)\n",
        "\n",
        "\n",
        "    completion = client.chat.completions.create(**completion_params)\n",
        "        # Extract and return the content of the response\n",
        "    return completion.choices[0].message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "0QIhzEd8M_2X",
        "outputId": "e8598084-08f4-4684-9bce-1bfd6ff77003"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant\n"
          ]
        },
        {
          "data": {
            "text/markdown": "That sounds interesting! INFOR 452 must be a course related to information systems or information technology. How can I assist you with teaching this course?",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "custom_messages =  [\n",
        "{'role':'system', 'content':'You are friendly AI chatbot.'},\n",
        "{'role':'user', 'content':'Hi, I am teaching INFOR 452'}  ]\n",
        "\n",
        "custom_parameters = {\n",
        "    \"max_tokens\": 500,\n",
        "}\n",
        "\n",
        "response = get_completion_adjustpara(\n",
        "        prompt=None,\n",
        "        parameters=custom_parameters,\n",
        "        custom_messages=custom_messages\n",
        "    )\n",
        "\n",
        "# Print the response\n",
        "if response:\n",
        "    print(response.role)  # Should output 'assistant'\n",
        "    display(Markdown(response.content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "xdOEPDSNNvUA",
        "outputId": "65767484-2067-44ae-8945-4fd39d3eddab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant\n"
          ]
        },
        {
          "data": {
            "text/markdown": "I'm sorry, but I don't have access to that information. Can you please remind me what you are teaching?",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "custom_messages =  [\n",
        "{'role':'system', 'content':'You are friendly AI chatbot.'},\n",
        "{'role':'user', 'content':'Hi, can you remind me what am I teaching?'}  ]\n",
        "\n",
        "custom_parameters = {\n",
        "    \"max_tokens\": 500,\n",
        "}\n",
        "\n",
        "response = get_completion_adjustpara(\n",
        "        prompt=None,\n",
        "        parameters=custom_parameters,\n",
        "        custom_messages=custom_messages\n",
        "    )\n",
        "\n",
        "# Print the response\n",
        "if response:\n",
        "    print(response.role)  # Should output 'assistant'\n",
        "    display(Markdown(response.content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "tdUWlBAZN2Vc",
        "outputId": "e99eca59-0d5b-4db6-94b0-30b3707e3b8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant\n"
          ]
        },
        {
          "data": {
            "text/markdown": "You are teaching INFOR 452. It is helpful for students enrolled in the course to learn about information systems, data analysis, and related topics. If you have any specific questions or need assistance with anything related to the course, feel free to ask!",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "custom_messages =  [\n",
        "{'role':'system', 'content':'You are friendly AI chatbot'},\n",
        "{'role':'user', 'content':'Hi, I am teaching INFOR 452'},\n",
        "{'role':'assistant', 'content': \"That's great! How can I assist you with INFOR 452?.\"},\n",
        "{'role':'user', 'content':'Hi, can you remind what am I teaching?'}  ]\n",
        "\n",
        "custom_parameters = {\n",
        "    \"max_tokens\": 500,\n",
        "}\n",
        "\n",
        "response = get_completion_adjustpara(\n",
        "        prompt=None,\n",
        "        parameters=custom_parameters,\n",
        "        custom_messages=custom_messages\n",
        "    )\n",
        "\n",
        "# Print the response\n",
        "if response:\n",
        "    print(response.role)  # Should output 'assistant'\n",
        "    display(Markdown(response.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NuiPGToOhad"
      },
      "source": [
        "# Use langchain memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tyL4RZEO_k7",
        "outputId": "198afef2-2d59-4e83-d67c-66f08ec1105f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.17-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.34 (from langchain_community)\n",
            "  Downloading langchain_core-0.3.35-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain<1.0.0,>=0.3.18 (from langchain_community)\n",
            "  Downloading langchain-0.3.18-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.37)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.6)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.6 (from langchain<1.0.0,>=0.3.18->langchain_community)\n",
            "  Downloading langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.18->langchain_community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.18->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.18->langchain_community) (2.27.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.17-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.18-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.35-py3-none-any.whl (413 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m413.2/413.2 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain-text-splitters, langchain, langchain_community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.33\n",
            "    Uninstalling langchain-core-0.3.33:\n",
            "      Successfully uninstalled langchain-core-0.3.33\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.5\n",
            "    Uninstalling langchain-text-splitters-0.3.5:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.5\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.17\n",
            "    Uninstalling langchain-0.3.17:\n",
            "      Successfully uninstalled langchain-0.3.17\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.18 langchain-core-0.3.35 langchain-text-splitters-0.3.6 langchain_community-0.3.17 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.7.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puooKuDOORHK"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        ")\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_core.messages import SystemMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOtUDE46QD8B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-ueJ_SqPRz-",
        "outputId": "7a6a44b6-83e0-4ee9-ed14-b38ba7d8d57d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-29-65a5df5e9951>:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(\n",
            "<ipython-input-29-65a5df5e9951>:15: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
            "<ipython-input-29-65a5df5e9951>:18: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(\n"
          ]
        }
      ],
      "source": [
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=1,\n",
        "    max_tokens=500,\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{text}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define the memory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# Define the chain as a RunnableSequence\n",
        "chain = LLMChain(\n",
        "    memory = memory,\n",
        "    prompt = prompt,\n",
        "    llm = llm,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8s9MoEOVV8R",
        "outputId": "56fc90e8-28a5-4e54-93de-fe9e8eee4a6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': 'Hello! Nice to meet you. What subject do you teach in INFOR 452?', 'chat_history': [HumanMessage(content='Hello I am teaching infor 452', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello! Nice to meet you. What subject do you teach in INFOR 452?', additional_kwargs={}, response_metadata={})]}\n"
          ]
        }
      ],
      "source": [
        "response = chain.invoke({\"text\": \"Hello I am teaching infor 452\"})\n",
        "\n",
        "# Print the response\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rgoaX2_Y_kq",
        "outputId": "c530a618-d4e0-48a9-cdea-c242c1fde44b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! Nice to meet you. What subject do you teach in INFOR 452?\n"
          ]
        }
      ],
      "source": [
        "print(response['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUVB0GZVY7Ch",
        "outputId": "b10d5fa9-f486-477e-8063-1efd647669b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[HumanMessage(content='Hello I am teaching infor 452', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello! Nice to meet you. What subject do you teach in INFOR 452?', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "print(response['chat_history'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Z7kfzj8XSc7",
        "outputId": "7eee6b4a-2023-4805-f38d-9412385b3e09"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': 'Sure! You mentioned that you are teaching INFOR 452. INFOR 452 is a course that typically covers topics related to information management, decision support systems, and data analytics. It may also include discussions on databases, business intelligence, and data visualization. If you need any specific information or resources related to this course, feel free to ask!',\n",
              " 'chat_history': [HumanMessage(content='Hello I am teaching infor 452', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hello! That's great to hear. What topics are you covering in your INFOR 452 class?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Hello can you remind me what am I teaching ?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Sure! You mentioned that you are teaching INFOR 452. INFOR 452 is a course that typically covers topics related to information management, decision support systems, and data analytics. It may also include discussions on databases, business intelligence, and data visualization. If you need any specific information or resources related to this course, feel free to ask!', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"text\": \"Hello can you remind me what am I teaching ?\"})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KfUsO0drX0HE",
        "outputId": "a9b7a30d-8062-455f-ddfc-10fafe5397b4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Of course! Based on your initial message, you are teaching INFOR 452.'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"text\": \"Hello can you remind me what am I teaching ?\"})['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9JBqDoYXqSX",
        "outputId": "f4749ea8-5e17-4e43-8677-ce53e5ac070c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='Hello I am teaching infor 452', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello! That's great to hear. What topics are you covering in your INFOR 452 class?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello can you remind me what am I teaching ?', additional_kwargs={}, response_metadata={}), AIMessage(content='Sure! You mentioned that you are teaching INFOR 452. INFOR 452 is a course that typically covers topics related to information management, decision support systems, and data analytics. It may also include discussions on databases, business intelligence, and data visualization. If you need any specific information or resources related to this course, feel free to ask!', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello can you remind me what am I teaching ?', additional_kwargs={}, response_metadata={}), AIMessage(content='It seems like you may be experiencing some memory issues. You mentioned earlier that you are teaching INFOR 452, a course that covers information management, decision support systems, and data analytics. If you need further assistance or information about the course, feel free to ask.', additional_kwargs={}, response_metadata={})]) return_messages=True memory_key='chat_history'\n"
          ]
        }
      ],
      "source": [
        "print(memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCnukzZQ831E"
      },
      "source": [
        "#Build Chatbot with Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7C6LQiVfn8o",
        "outputId": "cd84d7dd-4e56-4df9-fbdf-e1ac092add7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.25.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.25.2-py3-none-any.whl (46.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.25.2 gradio-client-1.8.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.6 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio\n",
        "import gradio as gr\n",
        "from IPython.display import Image, display, HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaMuEiStbVfc"
      },
      "outputs": [],
      "source": [
        "def get_completion(model=\"gpt-3.5-turbo\", parameters=None, custom_messages=None):\n",
        "    # Use custom messages if provided, otherwise use the default\n",
        "    if custom_messages:\n",
        "        messages = custom_messages\n",
        "    else:\n",
        "        # Default messages\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are an AI assistant.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Hello\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "    # Define the default parameters\n",
        "    completion_params = {\n",
        "        \"model\": model,\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 1,  # Adjust randomness\n",
        "        \"top_p\": 1,  # Use nucleus sampling\n",
        "        \"frequency_penalty\": 0,  # Penalize repetition (-2.0 to 2.0)\n",
        "        \"presence_penalty\": 0,  # Penalize introducing new topics (-2.0 to 2.0)\n",
        "    }\n",
        "\n",
        "    # Update the default parameters with any custom parameters passed in\n",
        "    if parameters:\n",
        "        completion_params.update(parameters)\n",
        "    #print(completion_params)\n",
        "\n",
        "\n",
        "    completion = client.chat.completions.create(**completion_params)\n",
        "        # Extract and return the content of the response\n",
        "    return completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgCqQsWfX7qk"
      },
      "outputs": [],
      "source": [
        "def collect_messages(prompt):\n",
        "    context.append({'role':'user', 'content':f\"{prompt}\"})\n",
        "    response = get_completion(custom_messages = context)\n",
        "    context.append({'role':'assistant', 'content':f\"{response}\"})\n",
        "    return response\n",
        "\n",
        "context = [ {'role':'system', 'content':\"\"\"\n",
        "You are OrderBot, an automated service to collect orders for a pizza restaurant. \\\n",
        "You first greet the customer, then collects the order, \\\n",
        "and then asks if it's a pickup or delivery. \\\n",
        "You wait to collect the entire order, then summarize it and check for a final \\\n",
        "time if the customer wants to add anything else. \\\n",
        "If it's a delivery, you ask for an address. \\\n",
        "Finally you collect the payment.\\\n",
        "Make sure to clarify all options, extras and sizes to uniquely \\\n",
        "identify the item from the menu.\\\n",
        "You respond in a short, very conversational friendly style. \\\n",
        "The menu includes \\\n",
        "pepperoni pizza  12.95, 10.00, 7.00 \\\n",
        "cheese burger 8.99, 12.99\\\n",
        "cheese pizza   10.95, 9.25, 6.50 \\\n",
        "eggplant pizza   11.95, 9.75, 6.75 \\\n",
        "fries 4.50, 3.50 \\\n",
        "greek salad 7.25 \\\n",
        "Toppings: \\\n",
        "extra cheese 2.00, \\\n",
        "mushrooms 1.50 \\\n",
        "sausage 3.00 \\\n",
        "canadian bacon 3.50 \\\n",
        "AI sauce 1.50 \\\n",
        "peppers 1.00 \\\n",
        "Drinks: \\\n",
        "coke 3.00, 2.00, 1.00 \\\n",
        "sprite 3.00, 2.00, 1.00 \\\n",
        "bottled water 5.00 \\\n",
        "\"\"\"} ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rFWJDzyP3G-b",
        "outputId": "db4f6942-3407-4814-e786-84328e3ab657"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! Welcome to our pizza restaurant. What would you like to order today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "collect_messages(\"hello\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "WucUFbMv3smW",
        "outputId": "cf1ecbf2-4e0b-4ab2-ac2a-bf52186b38ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Great choice! Which pizza would you like to order? We have pepperoni pizza for $12.95, cheese pizza for $10.95, and eggplant pizza for $11.95 in various sizes.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "collect_messages(\"hello I would like to order a pizza\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "3bjwnrPM3wir",
        "outputId": "3fce9239-1492-44ba-b067-b5d212ea9aec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Of course! You mentioned you'd like to order a pizza. Which pizza would you like to have: pepperoni pizza for $12.95, cheese pizza for $10.95, or eggplant pizza for $11.95? And in what size?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "collect_messages(\"Do you remember what I would like to order ?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "V5A_DF8z1mWJ",
        "outputId": "37f6a05a-63ec-47d3-fd14-9ec306df76d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://1d52194a32bcf8aa46.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1d52194a32bcf8aa46.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://1d52194a32bcf8aa46.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "\n",
        "def chatfunction(message, history):\n",
        "    print(message)\n",
        "    print(history)\n",
        "    print(message[\"text\"])\n",
        "    return collect_messages(message[\"text\"])\n",
        "\n",
        "demo = gr.ChatInterface(fn=chatfunction, type=\"messages\", title=\"Customer Order Chatbot\", multimodal=True) #examples=[\"pizza, drinks\"]\n",
        "\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boN4RoKPmniI",
        "outputId": "754870e5-5567-402a-ca2d-6ef43031ce74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "demo.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/Infor452/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hN3yXNdgNMX",
        "outputId": "910cfbea-60b4-4c86-ea21-30be6d4dc2ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Advanced prompt engineering.ipynb'    documents\t    serper_key.txt\n",
            "'Basics of prompt engineering.ipynb'   image.jpg\t    storage\n",
            " Bitcoin.png\t\t\t      'INFO 452 RAG.pptx'   Tech_Products.txt\n",
            " chroma_db\t\t\t       openai_key.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall -y uvloop\n",
        "\n",
        "#import asyncio\n",
        "#import sys\n",
        "#if \"uvloop\" in sys.modules:\n",
        "    #import importlib\n",
        "    #importlib.reload(asyncio)\n",
        "\n",
        "#import nest_asyncio\n",
        "#nest_asyncio.apply()\n"
      ],
      "metadata": {
        "id": "x3-mO5SjBgxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "WxHEpDoQEd9S",
        "outputId": "b149d165-d2bb-47da-a761-30d6d8efe830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://711e69a5ca52a3db64.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://711e69a5ca52a3db64.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Order pizza\n",
            "[]\n",
            "tell me about your menu\n",
            "[{'role': 'user', 'metadata': None, 'content': 'Order pizza', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Sure thing! Which pizza and size would you like to order: pepperoni pizza for $12.95, cheese pizza for $10.95, or eggplant pizza for $11.95? And do you want any toppings on that?', 'options': None}]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def chatfunction(message, history):\n",
        "    print(message)\n",
        "    print(history)\n",
        "    #print(message[\"text\"]) ###when using gr.ChatInterface Inside gr.Blocks this is not a dictionary but a string so you need to change the message[\"text\"] to message only !!!\n",
        "    return collect_messages(message)\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    # Add an image as a header\n",
        "    gr.Image(\n",
        "    \"/content/drive/MyDrive/Infor452/image.jpg\",\n",
        "    label=\"shopping cart\",\n",
        "    show_label=False,\n",
        "    width=300,  # Width in pixels\n",
        "    height=300  # Height in pixels\n",
        "    )\n",
        "\n",
        "    # Title with larger font size and color customization\n",
        "    gr.Markdown(\n",
        "        \"<h1 style='text-align: center; color: #800080;'>Customer Order Chatbot</h1>\"\n",
        "    )\n",
        "    # Blue color: #0000FF\n",
        "    # Green color: #4CAF50\n",
        "\n",
        "    # Add some instructions below the title\n",
        "    gr.Markdown(\n",
        "        \"<p style='text-align: center; font-size: 18px;'>Chat with our AI Service to place orders or ask questions about our offerings!</p>\"\n",
        "    )\n",
        "\n",
        "    # Add a row for the chat interface\n",
        "    with gr.Row():\n",
        "        gr.ChatInterface(\n",
        "            fn=chatfunction,\n",
        "            type=\"messages\",\n",
        "            examples=[\"Order pizza\", \"Ask about drinks\", \"Do you have vegan options?\"] ##change here\n",
        "        )\n",
        "\n",
        "# Launch the interface\n",
        "demo.launch(debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQiZXM3h65gI",
        "outputId": "df636dd6-55e5-4ae1-92c5-52a27b1883c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "demo.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N9glElAKUIE"
      },
      "source": [
        "#Prompt chaining\n",
        "\n",
        "##Implement a complex task with multiple prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6F7MNZ0SCXdL"
      },
      "outputs": [],
      "source": [
        "def get_completion(model=\"gpt-3.5-turbo\", parameters=None, custom_messages=None):\n",
        "    # Use custom messages if provided, otherwise use the default\n",
        "    if custom_messages:\n",
        "        messages = custom_messages\n",
        "    else:\n",
        "        # Default messages\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are an AI assistant.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Hello\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "    # Define the default parameters\n",
        "    completion_params = {\n",
        "        \"model\": model,\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 1,  # Adjust randomness\n",
        "        \"top_p\": 1,  # Use nucleus sampling\n",
        "        \"frequency_penalty\": 0,  # Penalize repetition (-2.0 to 2.0)\n",
        "        \"presence_penalty\": 0,  # Penalize introducing new topics (-2.0 to 2.0)\n",
        "    }\n",
        "\n",
        "    # Update the default parameters with any custom parameters passed in\n",
        "    if parameters:\n",
        "        completion_params.update(parameters)\n",
        "    #print(completion_params)\n",
        "\n",
        "\n",
        "    completion = client.chat.completions.create(**completion_params)\n",
        "        # Extract and return the content of the response\n",
        "    return completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9r84mqM9FAt"
      },
      "outputs": [],
      "source": [
        "menu = \"\"\"\n",
        "The menu includes \\\n",
        "pepperoni pizza  12.95, 10.00, 7.00 \\\n",
        "cheese pizza   10.95, 9.25, 6.50 \\\n",
        "eggplant pizza   11.95, 9.75, 6.75 \\\n",
        "fries 4.50, 3.50 \\\n",
        "greek salad 7.25 \\\n",
        "Toppings: \\\n",
        "extra cheese 2.00, \\\n",
        "mushrooms 1.50 \\\n",
        "sausage 3.00 \\\n",
        "canadian bacon 3.50 \\\n",
        "AI sauce 1.50 \\\n",
        "peppers 1.00 \\\n",
        "Drinks: \\\n",
        "coke 3.00, 2.00, 1.00 \\\n",
        "sprite 3.00, 2.00, 1.00 \\\n",
        "bottled water 5.00 \\\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emsSOluzKTnK",
        "outputId": "4bd504da-c2b5-4b2b-f03c-176a15309482"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "product,price,reviews\n",
            "eggplant pizza,11.95,\"Delicious and flavorful eggplant pizza, definitely a must-try! The combination of eggplant and cheese is simply mouth-watering.\"\n",
            "eggplant pizza,9.75,\"I was pleasantly surprised by how tasty the eggplant pizza was. The crust was perfectly crispy and the eggplant topping was cooked to perfection.\"\n",
            "cheese pizza,10.95,\"The cheese pizza was disappointing, the cheese tasted bland and the crust was not as good as expected. Would not recommend.\"\n",
            "cheese pizza,9.25,\"The cheese pizza lacked in flavor and the cheese was not evenly spread. Overall, a subpar experience with the cheese pizza.\"\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "With the menu provided and delimited with |, you are required to:\n",
        "1. Write 2 positive reviews for eggplant pizza.\n",
        "2. Write 2 negative reviews about cheese pizza.\n",
        "3. Convert the reviews into a CSV format with the following columns:\n",
        "   - product\n",
        "   - price\n",
        "   - reviews\n",
        "\n",
        "Menu:\n",
        "|{menu}|\n",
        "\"\"\"\n",
        "custom_messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You will be provided with unstructured data, and your task is to parse it into CSV format\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "        }\n",
        "    ]\n",
        "\n",
        "custom_parameters = {\n",
        "    \"temperature\": 1,\n",
        "    \"max_tokens\": 500,\n",
        "}\n",
        "\n",
        "response = get_completion(\n",
        "    parameters=custom_parameters,\n",
        "    custom_messages=custom_messages\n",
        ")\n",
        "\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8c-pDlRImgI"
      },
      "outputs": [],
      "source": [
        "def alternative_suggstions(conversation):\n",
        "  prompt = f\"\"\"\n",
        "  The conversation is delimited with |\n",
        "  Table:\n",
        "  product,price,reviews\n",
        "  eggplant pizza,11.95,\"Delicious and flavorful, the eggplant pizza exceeded my expectations. The combination of eggplant and cheese was simply perfect.\"\n",
        "  eggplant pizza,9.75,\"The eggplant pizza was a delightful surprise. The crust was crispy, and the eggplant topping was cooked to perfection.\"\n",
        "  cheese pizza,10.95,\"The cheese pizza was disappointing as the cheese was not melted properly and the crust was soggy. Would not recommend.\"\n",
        "  cheese pizza,9.25,\"The cheese pizza was lacking in flavor and the cheese seemed old. Not worth the price.\"\n",
        "\n",
        "  |{conversation}|\n",
        "  \"\"\"\n",
        "  custom_messages = [\n",
        "      {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"You will be provided with a conversation between an orderbot and a customer. \\\n",
        "          Your task is as follows: \\\n",
        "          Identify whether the consumer ordered any items from the table provided. \\ For each ordered item, classify the associated reviews as either positive or negative. \\\n",
        "          If the consumer ordered an item with negative reviews, create a proper instruction suggesting the chatbot that it has negative review and offer alternative items to the consumer. \\\n",
        "          If the consumer ordered an item with positive reviews, create a proper instruction suggesting the chatbot to encourage the customer to order more postive items. \\\n",
        "          If the consumer do not order anything from the table, create a proper instruction suggesting the chatbot to encourage the customer to order more items\\\n",
        "          The final answer should only include a proper isntruction.\"\n",
        "      },\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": prompt\n",
        "      }\n",
        "  ]\n",
        "\n",
        "  custom_parameters = {\n",
        "      \"temperature\": 0.5,\n",
        "      \"max_tokens\": 258,\n",
        "  }\n",
        "\n",
        "  response = get_completion(\n",
        "      parameters=custom_parameters,\n",
        "      custom_messages=custom_messages\n",
        "  )\n",
        "  #print(\"Alternative Suggestions Debug:\\n\", response)\n",
        "\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ebfd530NLQAt"
      },
      "outputs": [],
      "source": [
        "conversation = \"\"\"\n",
        "I would like to order cheese pizza\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO6MWbRnNeVt",
        "outputId": "eb1e91c2-3bca-49c6-cf7b-ca893bde4382"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The customer ordered \"cheese pizza\" which has negative reviews. Please suggest alternative items to the customer.\n"
          ]
        }
      ],
      "source": [
        "print(alternative_suggstions(conversation))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8hHCdDPFZzl"
      },
      "outputs": [],
      "source": [
        "def collect_messages_new(prompt):\n",
        "    # Get the first instruction based on the prompt\n",
        "    first_instruction = alternative_suggstions(prompt)\n",
        "    #print(first_instruction)\n",
        "    # Combine the instruction with the user prompt\n",
        "    final_prompt = f\"\"\"You are provided with a new instruction delimited by |, followed by a consumer prompt.| {first_instruction} || Consumer Prompt: {prompt}\"\"\"\n",
        "    #print(final_prompt)\n",
        "    # Append the final prompt to the context as a user message\n",
        "    context.append({'role': 'user', 'content': final_prompt})\n",
        "    # Generate the response using the context\n",
        "    response = get_completion(custom_messages=context)\n",
        "\n",
        "    # Append the assistant's response to the context\n",
        "    context.append({'role': 'assistant', 'content': response})\n",
        "\n",
        "    # Return the assistant's response\n",
        "    return response\n",
        "\n",
        "context = [ {'role':'system', 'content':\"\"\"\n",
        "You are OrderBot, an automated service to collect orders for a pizza restaurant. \\\n",
        "You first greet the customer, then collects the order, \\\n",
        "and then asks if it's a pickup or delivery. \\\n",
        "You wait to collect the entire order, then summarize it and check for a final \\\n",
        "time if the customer wants to add anything else. \\\n",
        "If it's a delivery, you ask for an address. \\\n",
        "Finally you collect the payment.\\\n",
        "Make sure to clarify all options, extras and sizes to uniquely \\\n",
        "identify the item from the menu.\\\n",
        "You respond in a short, very conversational friendly style. \\\n",
        "The menu includes \\\n",
        "pepperoni pizza  12.95, 10.00, 7.00 \\\n",
        "cheese pizza   10.95, 9.25, 6.50 \\\n",
        "eggplant pizza   11.95, 9.75, 6.75 \\\n",
        "fries 4.50, 3.50 \\\n",
        "greek salad 7.25 \\\n",
        "Toppings: \\\n",
        "extra cheese 2.00, \\\n",
        "mushrooms 1.50 \\\n",
        "sausage 3.00 \\\n",
        "canadian bacon 3.50 \\\n",
        "AI sauce 1.50 \\\n",
        "peppers 1.00 \\\n",
        "Drinks: \\\n",
        "coke 3.00, 2.00, 1.00 \\\n",
        "sprite 3.00, 2.00, 1.00 \\\n",
        "bottled water 5.00 \\\n",
        "In some situations you would be provide an instruction to offer alternative then suggestions consumer nicely\n",
        "\"\"\"} ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "LtKCWcZzPaBM",
        "outputId": "c11131c5-d4de-4ea4-c242-4d98ff6e1fd5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Hey there! ðŸ• It's great to hear from you! Just a heads up, the cheese pizza has received some negative feedback. Might I suggest trying our delicious pepperoni or eggplant pizza instead? They are very popular choices on our menu! What would you like to go for? ðŸ˜Š\""
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "collect_messages_new(\"I would like to order cheese pizza\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "GLHzzwOKO5tM",
        "outputId": "98732251-7b80-4736-b068-910255b75c51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7862, \"/\", \"100%\", 500, false, window.element)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def chatfunction(message, history):\n",
        "    print(message)\n",
        "    print(history)\n",
        "    print(message[\"text\"])\n",
        "    return collect_messages_new(message[\"text\"])\n",
        "\n",
        "demo = gr.ChatInterface(fn=chatfunction, type=\"messages\", title=\"Customer Order Chatbot\", multimodal=True) #examples=[\"pizza\"],\n",
        "demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJrjbp_eGWgh",
        "outputId": "ec1c8c47-bc66-4e5a-cffa-0cd37c3d5380"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "demo.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}